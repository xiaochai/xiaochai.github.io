---
layout: post
title: "机器学习课程备忘"
date: 2017-5-12
categories:
  - Tech
description: 
image: http://wx4.sinaimg.cn/large/6a1f6674ly1ffcsacbkv8j21io10g7w2.jpg
image-sm: http://wx4.sinaimg.cn/mw1024/6a1f6674ly1ffcsacbkv8j21io10g7w2.jpg
---
<style>
.myMJSmall {
	font-size: 0.8em;
}
</style>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

#### 机器学习定义

*Arthur Samuel(1959), informal definition:* 

> the field of study that gives computers the ability to learn without being explicitly programmed.

*Tom Mitchell(1998), modern definition:*

> A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.

#### 机器学习分类

##### 监督学习(Supervised learning)

*定义：给出的机器学习样本集含有正确的答案（标签）*

*细分*

> 回归问题(Regression): 预测值是连续的 ====> 房价预测问题<br/>
> 分类问题(Classification): 预测值为离散 ====> 恶性和良性肿瘤问题

##### 无监督学习(Unsupervised learning)

*定义：给出的样本集没有标签*

*细分*

> 聚类算法(Clustering): 谷歌新闻汇聚 <br/>
> 非聚类算法(Non-clustering): 鸡尾酒会问题（Cocktail Party Algorithm）[[W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x');]

##### 其它

> reinforcement learning, recommender systems

#### 单变量线性回归(Linear Regression with One Variable)

##### 符号

> m: 样本数量<br/>
> x: 输入变量/特征<br/>
> y: 输出变量/目标变量<br/>
> (x,y): 一个样本<br/>
> $$(x^{(i)}, y^{(i)})$$: 第i行样本，有时写做$$(x_i, y_i)$$

##### 预测函数(hypothesis)

$$\class{myMJSmall}{h_\theta(x)=\theta_0+\theta_1*x}$$


##### 代价函数(cost function/Squared error function/Mean squared error)

$$\class{myMJSmall}{J(\theta_0,\theta_1)=\frac 1{2m}\sum_{i=1}^m(h_\theta(x_i)-y_i)^2}$$


##### 梯度下降(Gradient descent)

$$\class{myMJSmall}{\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)}$$

> $$\alpha$$: learning rate(选择过大或者过小时的情况)

*对于单变量线性回归，使用这个公式同时更新$$\theta_0$$和$$\theta_1$$的值，直到$$(J(\theta_0, \theta_1))$$最小（收敛）*


*当j=0时*

$$\class{myMJSmall}{\frac{\partial}{\partial \theta_0} J(\theta_0, \theta_1) = \frac{1}{m} \sum_{i=1}^m(h_\theta(x_i)-y_i)}$$

*当j=1时*

$$\class{myMJSmall}{\frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1) = \frac{1}{m} \sum_{i=1}^m(h_\theta(x_i)-y_i)x_i}$$

##### 证明: 两个函数相加的导数等于两个函数的导数相加$$(f(x)+g(x))' = f'(x) + g'(x)$$


> 根据定义$$f'(x) = lim_{\Delta x\rightarrow0}\frac{f(x+\Delta x)-f(x)}{\Delta x} \\$$
> 所以 $$(f(x)+g(x))' = lim_{\Delta x\rightarrow0}\frac{f(x+\Delta x)+g(x+\Delta x)-(f(x)+g(x))}{\Delta x} \\
> f'(x)+g'(x) = lim_{\Delta x\rightarrow0}\frac{f(x+\Delta x)-f(x)}{\Delta x} + lim_{\Delta x\rightarrow0}\frac{g(x+\Delta x)-g(x)}{\Delta x} \\
>             = lim_{\Delta x\rightarrow0}\frac{f(x+\Delta x)-f(x)+g(x+\Delta x)-g(x)}{\Delta x} \\$$
> 所以 $$(f(x)+g(x))' = f'(x)+g'(x)$$


##### 链式求导法则:$$(f(g(x)))' = f'(g(x))g'(x)$$ or $$\frac{dy}{dx} = \frac{dy}{dz}\frac{dz}{dx} $$

##### 证明：$$ \class{myMJSmall}{\frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1) = \frac{1}{m} \sum_{i=1}^m(h_\theta(x_i)-y_i)x_i}$$

> 证明如下：$$\frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1) = \frac{\partial}{\partial \theta_1} \frac{1}{2m} \sum_{i=1}^m(h_\theta(x_i)-y_i)^2 \\
> = \frac{\partial}{\partial \theta_1} \frac{1}{2m} \sum_{i=1}^m(\theta_0+\theta_1 x_i-y_i)^2 \\
> = \frac{\partial}{\partial \theta_1} \frac{1}{2m} ((\theta_0+\theta_1 x_1 -y_1)^2+ \cdots + (\theta_0+\theta_1 x_m - y_m)^2) \\
> = \frac{\partial}{\partial \theta_1} \frac{1}{2m} (\theta_0+\theta_1 x_1 -y_1)^2 + \cdots + \frac{\partial}{\partial \theta_1} \frac{1}{2m} (\theta_0+\theta_1 x_m -y_m)^2 \\ 
> = \frac{\partial(\frac{1}{2m}(\theta_0+\theta_1 x_1 -y_1)^2)}{\partial(\theta_0+\theta_1 x_1 -y_1)} \cdot \frac{\partial(\theta_0+\theta_1 x_1 -y_1)}{\partial\theta_1} + \cdots + \frac{\partial(\frac{1}{2m}(\theta_0+\theta_1 x_m -y_m)^2)}{\partial(\theta_0+\theta_1 x_m -y_m)} \cdot \frac{\partial(\theta_0+\theta_1 x_m -y_m)}{\partial\theta_1}\\
> = \frac{1}{m}(\theta_0+\theta_1 x_1 - y_1) \cdot x_1 + \cdots + \frac{1}{m}(\theta_0 + \theta_1 x_m - y_m) \cdot x_m \\
> = \frac{1}{m}\sum_{i=1}^m(\theta_0+\theta_1 x_i+y_i)x_i \\
> = \frac{1}{m}\sum_{i=1}^m(h_\theta(x_i) - y_i)x_i
> $$


##### 代价函数的图像

*线性回归的代价函数($$J(\theta)$$) 总是一个碗形(bow shaped function)，这种函数叫做凸函数(convex function)*

*批量梯度下降(Batch gradient descent): 即每一次迭代$$\theta$$都需要计算全部的样本数据*


##### 学习资料

[课件和笔记](/assets/material/ml/)
